# LLM Chat UI

Веб-интерфейс для взаимодействия с языковыми моделями через Ollama.

## Особенности

- Множественные чаты с историей сообщений
- Настройка параметров для каждой модели (температура, длина ответа)
- Шаблоны системных промптов для специализированных ролей
- Потоковый вывод ответов модели в реальном времени через WebSocket
- Адаптивная работа с SQLite (локальная разработка) или PostgreSQL (продакшен)
- Docker и Docker Compose для простого развертывания

## Встроенные шаблоны ролей

Приложение поставляется с несколькими предустановленными шаблонами ролей:

1. **Личный ассистент Артём** - дружелюбный ассистент по умолчанию для повседневных задач
2. **Python Developer** - опытный разработчик для помощи с кодом на Python и другими технологиями
3. **IT Mentor** - наставник для обучения программированию и ИТ-концепциям
4. **Юморист** - собеседник с отличным чувством юмора

Вы можете легко добавить свои собственные шаблоны через веб-интерфейс.

## Требования

- Python 3.8+
- Ollama (https://github.com/ollama/ollama) с установленными моделями
- Для разработки: SQLite (уже включен)
- Для продакшена: PostgreSQL
- Docker и Docker Compose (опционально, для контейнеризации)

## Установка и запуск

### Локальная разработка (SQLite)

1. Клонировать репозиторий:
```
git clone https://github.com/vebulogmetra/chat-ui
cd chat-ui
```

2. Создать виртуальное окружение:
```
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# или
.venv\Scripts\activate     # Windows
```

3. Установить зависимости:
```
pip install -r requirements.txt
```

4. Создать файл .env на основе .env.example:
```
cp .env.example .env
```

5. Убедитесь, что Ollama запущен и доступен:
```
curl http://localhost:11434/api/version
```

6. Запустить приложение:
```
python run.py
```

7. Открыть в браузере адрес http://localhost:8000

### Запуск с Docker Compose (PostgreSQL)

1. Клонировать репозиторий:
```
git clone https://github.com/vebulogmetra/chat-ui
cd chat-ui
```

2. Создать .env файл:
```
cp .env.example .env
```

3. Настроить переменные окружения в .env для PostgreSQL:
```
DATABASE_URL=postgresql+asyncpg://postgres:postgres@db:5432/chat-ui
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=chat-ui
OLLAMA_API_URL=http://ollama:11434/api
```

4. Запустить контейнеры:
```
docker-compose up -d
```

5. Открыть в браузере адрес http://localhost:8000

## Тестирование

Проект включает набор автоматических тестов для backend-части:

```
# Запуск всех тестов
pytest tests/

# Запуск тестов с подробным выводом
pytest tests/ -v

# Запуск конкретного тестового файла
pytest tests/test_models.py
```

## Использование

- **Создание нового чата**: Нажмите кнопку "Новый чат" в боковой панели
- **Выбор модели**: Выберите модель из выпадающего списка внизу чата
- **Настройка модели**: Нажмите иконку шестеренки рядом с выбором модели
- **Выбор роли для модели**: Выберите готовый шаблон из списка и нажмите "Применить"
- **Отправка сообщения**: Введите текст в поле внизу и нажмите Enter или кнопку отправки
- **Сохранение настроек**: Настройки автоматически сохраняются для каждой модели
- **Удаление чата**: Нажмите кнопку "X" рядом с названием чата в боковой панели

## Структура проекта

```
.
├── app/                    # Основной пакет приложения
│   ├── database/           # Модули для работы с базой данных
│   ├── models/             # Модели данных
│   ├── routes/             # Маршруты API и веб-сокеты
│   ├── schemas/            # Схемы валидации (Pydantic)
│   ├── services/           # Сервисы для работы с API Ollama
│   ├── static/             # Статические файлы (CSS, JS)
│   └── templates/          # HTML шаблоны
├── tests/                  # Тесты приложения
│   ├── conftest.py         # Конфигурация тестов
│   ├── test_models.py      # Тесты моделей данных
│   └── test_routes.py      # Тесты маршрутов API
├── .env.example            # Пример файла с переменными окружения
├── docker-compose.yml      # Настройки Docker Compose
├── Dockerfile              # Dockerfile для сборки образа
├── pyproject.toml          # Конфигурация Python-проектов и тестов
├── main.py                 # Точка входа для запуска через uvicorn
├── requirements.txt        # Зависимости проекта
└── run.py                  # Скрипт для запуска приложения в режиме разработки
```

## Особенности реализации

- **Потоковая передача ответов**: WebSocket для передачи ответов модели в реальном времени
- **Система шаблонов**: Предопределенные шаблоны ролей с системными и пользовательскими промптами
- **Адаптивная база данных**: Автоматическое определение типа БД на основе конфигурации
- **Модуль настройки моделей**: Индивидуальные параметры для каждой языковой модели
- **Форматирование сообщений**: Поддержка Markdown и подсветка синтаксиса в блоках кода

## Лицензия

MIT