# LLM Chat UI

Веб-интерфейс для взаимодействия с языковыми моделями через Ollama.

## Особенности

- Множественные чаты с историей
- Поддержка всех моделей, доступных в Ollama
- Настройка параметров для каждой модели (температура, длина ответа)
- Шаблоны системных промптов для специализированных ролей
- Интерфейс в стиле ChatGPT
- Потоковый вывод ответов модели
- Поддержка Docker и Docker Compose
- Работа с SQLite (разработка) или PostgreSQL (продакшен)

## Встроенные шаблоны ролей

Приложение поставляется с несколькими предустановленными шаблонами ролей:

1. **Личный ассистент Артём** - дружелюбный ассистент по умолчанию
2. **Python Developer** - опытный разработчик для помощи с кодом на Python
3. **IT Mentor** - наставник для обучения программированию и ИТ-концепциям
4. **Юморист** - собеседник с отличным чувством юмора

Вы можете легко добавить свои собственные шаблоны через интерфейс.

## Требования

- Python 3.8+
- Ollama (https://github.com/ollama/ollama) с установленными моделями
- Для разработки: SQLite
- Для продакшена: PostgreSQL
- Docker и Docker Compose (опционально)

## Установка и запуск

### Локальная разработка

1. Клонировать репозиторий:
```
git clone https://github.com/yourusername/llm-chat-ui.git
cd llm-chat-ui
```

2. Создать виртуальное окружение:
```
python -m venv venv
source venv/bin/activate  # На Windows: venv\Scripts\activate
```

3. Установить зависимости:
```
pip install -r requirements.txt
```

4. Создать файл .env на основе .env.example:
```
cp .env.example .env
```

5. Запустить приложение:
```
python run.py
```

6. Открыть в браузере адрес http://localhost:8000

### Запуск с Docker Compose

1. Клонировать репозиторий:
```
git clone https://github.com/yourusername/llm-chat-ui.git
cd llm-chat-ui
```

2. Создать .env файл:
```
cp .env.example .env
```

3. Настроить переменные окружения в .env для PostgreSQL:
```
POSTGRES_USER=postgres
POSTGRES_PASSWORD=yourpassword
POSTGRES_DB=llm_chat
OLLAMA_API_URL=http://ollama:11434/api
```

4. Запустить контейнеры:
```
docker-compose up -d
```

5. Открыть в браузере адрес http://localhost:8000

## Тестирование

Проект включает набор автоматических тестов для backend-части:

```
# Запуск всех тестов
pytest tests/

# Запуск тестов с подробным выводом
pytest tests/ -v

# Запуск конкретного тестового файла
pytest tests/test_models.py
```

## Использование

- **Создание нового чата**: Нажмите кнопку "Новый чат" в боковой панели
- **Выбор модели**: Выберите модель из выпадающего списка внизу чата
- **Настройка модели**: Нажмите иконку шестеренки рядом с выбором модели
- **Выбор роли для модели**: Выберите готовый шаблон из списка и нажмите "Применить"
- **Сохранение настроек**: Настройки автоматически сохраняются для каждой модели
- **Удаление чата**: Нажмите кнопку "X" рядом с названием чата в боковой панели

## Структура проекта

```
.
├── app/                    # Основной пакет приложения
│   ├── database/           # Модули для работы с базой данных
│   ├── models/             # Модели данных
│   ├── routes/             # Маршруты API
│   ├── services/           # Сервисы (работа с Ollama API)
│   ├── static/             # Статические файлы (CSS, JS)
│   └── templates/          # HTML шаблоны
├── tests/                  # Тесты
│   ├── conftest.py         # Конфигурация тестов
│   ├── test_models.py      # Тесты моделей данных
│   └── test_routes.py      # Тесты маршрутов API
├── .env.example            # Пример файла с переменными окружения
├── docker-compose.yml      # Настройки Docker Compose
├── Dockerfile              # Dockerfile для сборки образа
├── main.py                 # Основной файл FastAPI приложения
├── requirements.txt        # Зависимости проекта
└── run.py                  # Скрипт для запуска приложения
```

## Разработка

Для добавления новых функций:

1. Добавьте маршруты в app/routes/
2. Обновите модели в app/models/models.py
3. Обновите сервисы в app/services/
4. Добавьте HTML шаблоны в app/templates/
5. Напишите JavaScript код в app/static/js/
6. Добавьте тесты в tests/

## Лицензия

MIT 